<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>VQA: Visual Question Answering</title>
    <link rel="stylesheet" href="./static/css/foundation.css">
    <link rel="stylesheet" href="./static/css/main.css">
</head>

<body class="off-canvas hide-extras" style="min-width:1200px; min-height:750px;">
    <header>
        <div class="row">
            <a href="http://visualqa.org/"><img style="height: 100px; position:absolute; top:4px; left:0px;" src="./static/img/main.png" alt="logo" /></a>
            <h1><img style="height: 90px;" src="./static/img/logo.png" alt="logo" /><br></h1>
            <br>
        </div>
    </header>
    <div class="contain-to-grid">
        <nav class="top-bar" data-topbar>
            <section class="top-bar-section">
                <!-- Right Nav Section -->
                <ul class="right">
                    <li><a href="index.html">Home</a></li>
                    <li><a href="people.html">People</a></li>
                    <li><a href="code.html">Code</a></li>
                    <li><a href="http://vqa.cloudcv.org/" onClick="ga('send', 'event', { eventCategory: 'Outgoing Link', eventAction: 'Demo', eventLabel: 'Demo'});">Demo</a></li>
                    <li class="has-dropdown"><a href="download.html">Download</a>
                        <ul class="dropdown">
                            <li><a href="download.html">VQA v2</a></li>
                            <li><a href="vqa_v1_download.html">VQA v1</a></li>
                        </ul>
                    </li>
                    <li><a href="evaluation.html">Evaluation</a></li>
                    <li class="has-dropdown"><a href="challenge.html">Challenge</a>
                        <ul class="dropdown">
                             <li><a href="challenge.html">2021</a></li>
                            <li><a href="challenge_2020.html">2020</a></li>
                            <li><a href="challenge_2019.html">2019</a></li><li><a href="challenge_2018.html">2018</a></li>
                            <li><a href="challenge_2017.html">2017</a></li>
                            <li><a href="challenge_2016.html">2016</a></li>
                        </ul>
                    </li>
                    <li class="has-dropdown"><a href="http://visualqa.org/vqa_v2_teaser.html">Browse</a>
                        <ul class="dropdown">
                            <li><a href="http://visualqa.org/vqa_v2_teaser.html">VQA v2</a></li>
                            <li><a href="https://vqabrowser.cloudcv.org/">VQA v1</a></li>

                        </ul>
                    </li>
                    <li><a href="http://visualqa.org/visualize/">Visualize</a></li>
                    <!--                     <li class="has-dropdown"><a href="http://visualqa.org/visualize/">Visualize</a>
                        <ul class="dropdown">
                            <li><a href="http://visualqa.org/visualize/">VQA v2</a></li>
                            <li><a href="http://visualqa.org/visualize/">VQA v1</a></li>
                        </ul>
                    </li> -->
                    <li class="has-dropdown"><a href="workshop.html">Workshop</a>
                        <ul class="dropdown">
                            <li><a href="workshop.html">2021</a></li>
                            <li><a href="workshop_2020.html">2020</a></li>
                            <li><a href="workshop_2019.html">2019</a></li>
                            <li><a href="workshop_2018.html">2018</a></li>
                            <li><a href="workshop_2017.html">2017</a></li>
                            <li><a href="workshop_2016.html">2016</a></li>
                        </ul>
                    </li>
                    <li><a href="sponsors.html">Sponsors</a></li>
                    <li><a href="terms.html">Terms</a></li>
                    <li><a href="external.html">External</a></li>
                </ul>
            </section>
        </nav>
    </div>
    <section role="main" style="padding: 1em;">
        <div class="row">
            <p style="font-size:30px; color:black; font-weight: 50" align=center>VQA Challenge Workshop
                <br>
                <span style="font-size:20px; color:black; font-weight: 400" align=center>Location: <b>Room 301AB, Hawaii Convention Center</b></span>
                <br>
                <span style="font-size:18px; color:gray; font-weight: 50" align=center>at CVPR 2017, July 26, Honolulu, Hawaii, USA</span></p>
            <p style="font-size:20px; color:black; font-weight: 50" align=center><a href="workshop.html" style="padding:13px">Home</a> <a href="workshop.html#program" style="padding:13px">Program</a> <a href="workshop.html#sub" style="padding:13px">Submission</a><a href="abstracts.html" style="padding:13px">Accepted Abstracts</a>
            </p>
            <hr>
        </div>
        <div class="row">
            <h1 style="font-size:30px; color:grey; font-weight: 200">Accepted Abstracts</h1>
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 200; text-align:left">
                    <b>An Analysis of Visual Question Answering Algorithms</b>
                    <br> Kushal Kafle, Christopher Kanan
                    <br>
                    <br>
                    <b>What's in a Question: Using Visual Questions as a Form of Supervision</b>
                    <br> Siddha Ganju, Olga Russakovsky, Abhinav Gupta
                    <br>
                    <br>
                    <b>Reasoning about Fine-grained Attribute Phrases using Reference Games</b>
                    <br> Jong-Chyi Su*, Chenyun Wu*, Huaizu Jiang, Subhransu Maji
                    <br>
                    <br>
                    <b>Visual Discriminative Question Generation</b>
                    <br> Yining Li, Chen Huang, Chen Change Loy, Xiaoou Tang
                    <br>
                    <br>
                    <b>Zero-Shot Visual Question Answering</b>
                    <br> Damien Teney, Anton van den Hengel
                    <br>
                    <br>
                    <b>MUTAN 2.0: Multimodal Tucker Fusion for Visual Question Answering</b>
                    <br> Hedi Ben-younes*, Remi Cadene*, Matthieu Cord, Nicolas Thome
                    <br>
                    <br>
                    <b>End-to-end optimization of goal-driven and visually grounded dialogue systems</b>
                    <br> Florian Strub, Harm de Vries, Jeremie Mary, Bilal Piot, Aaron Courville, Olivier Pietquin
                    <br>
                    <br>
                    <b>Visual Reference Resolution using Attention Memory for Visual Dialog</b>
                    <br> Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal
                    <br>
                    <br>
                    <b>FVQA: Fact-based Visual Question Answering</b>
                    <br> Peng Wang*, Qi Wu*, Chunhua Shen, Anthony Dick, Anton van den Hengel
                    <br>
                    <br>
                    <b>The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions</b>
                    <br> Peng Wang*, Qi Wu*, Chunhua Shen, Anton van den Hengel
                    <br>
                    <br>
                    <b>MemexQA: A Personal Question Answering Task</b>
                    <br> Lu Jiang, Liangliang Cao, KwakYannis Kalantidis, Sachin Farfade, Junwei Liang, Alexander Hauptmann
                    <br>
                    <br>
                    <b>TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering</b>
                    <br> Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim
                    <br>
                    <br>
                    <b>Vision and Reasoning base Image Riddles Answering through Probabilistic Soft Logic</b>
                    <br> Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis Aloimonos
                    <br>
                    <br>
                    <b>Towards Good Practices for Visual Question Answering</b>
                    <br> Zhe Wang, Xiaoyi Liu, Liangjian Chen, Limin Wang, Yu Qiao, Xiaohui Xie, Charless Fowlkes
                    <br>
                    <br>
                    <b>Compact Tensor Pooling for Visual Question Answering</b>
                    <br> Yang Shi, Tommaso Furlanello, Anima Anandkumar
                    <br>
                    <br>
                    <b>Attention Memory for Locating an Object through Visual Dialogue</b>
                    <br> Cheolho Han*, Yujung Heo*, Wooyoung Kang, Jaehyun Jun, Byoung-Tak Zhang
                    <br>
                    <br>
                    <b>VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation</b>
                    <br> Chuang Gan, Haoxiang Li, Chen Sun, Boqing Gong
                    <br>
                    <br>
                    <b>Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering</b>
                    <br> Zhou Yu, Jun Yu, Chenchao Xiang, Dalu Guo, Jianping Fan, Dacheng Tao
                    <br>
                    <br>
                    <b>VQABQ: Visual Question Answering by Basic Questions</b>
                    <br> Jia-Hong Huang, Modar Alfadly, Bernard Ghanem
                    <br>
                    <br>
                    <b>Hybrid Memory Enabled Neural Turing Machine for Visual Question Answering</b>
                    <br> Jun Zhang, Peng Xia, Yingxuan Zhu, Lifeng Liu, Xiaotian Yin, Jian Li
                    <br>
                    <br>
                    <b>On the Importance of Location for VQA: Stacked Twin Attention Networks</b>
                    <br> Haoqi Fan, Jiatong Zhou
                    <br>
                    <br>
                    <b>Creativity: Generating Diverse Questions using Variational Autoencoders</b>
                    <br> Unnat Jain*, Ziyu Zhang*, Alexander Schwing
                    <br>
                    <br>
                    <b>A Simple Loss Function for Improving the Convergence and Accuracy of Visual Question Answering Models</b>
                    <br> Ilija Ilievski, Jiashi Feng
                    <br>
                    <br>
                    <b> Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering</b>
                    <br> Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi
                </p>
            </div>
            <hr>
        </div>
    </section>
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-63638588-1', 'auto');
    ga('send', 'pageview');
    </script>
    <!-- jquery smooth scroll to id's -->
    <script>
    $(function() {
        $('a[href*=#]:not([href=#])').click(function() {
            if (location.pathname.replace(/^\//, '') == this.pathname.replace(/^\//, '') && location.hostname == this.hostname) {
                var target = $(this.hash);
                target = target.length ? target : $('[name=' + this.hash.slice(1) + ']');
                if (target.length) {
                    $('html,body').animate({
                        scrollTop: target.offset().top
                    }, 1000);
                    return false;
                }
            }
        });
    });
    </script>
</body>

</html>
