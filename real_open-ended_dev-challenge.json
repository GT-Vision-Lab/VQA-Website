[{"submissionRound": 4, "challenge": {"overall": 59.51, "perAnswerType": {"other": 45.89, "number": 36.97, "yes/no": 81.23}}, "dev": {"overall": 59.17, "perAnswerType": {"other": 45.23, "number": 38.42, "yes/no": 81.01}}, "standard": {"overall": 59.44, "perAnswerType": {"other": 45.83, "number": 37.12, "yes/no": 81.07}}, "team_name_order": 1, "team_members": "Qi Wu (Australia Centre for Visual Technologies, University of Adelaide), Peng Wang (Australia Centre for Visual Technologies, University of Adelaide), Chunhua Shen (Australia Centre for Visual Technologies, University of Adelaide), Anthony Dick (Australia Centre for Visual Technologies, University of Adelaide), Anton van den Hengel (Australia Centre for Visual Technologies, University of Adelaide)", "team_name": "ACVT_Adelaide", "ref": "http://arxiv.org/abs/1511.06973", "method": "We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible."}, {"submissionRound": 1, "challenge": {"overall": 55.88, "perAnswerType": {"other": 42.73, "number": 33.84, "yes/no": 76.92}}, "dev": {"overall": 55.72, "perAnswerType": {"other": 42.62, "number": 35.03, "yes/no": 76.55}}, "standard": {"overall": 55.89, "perAnswerType": {"other": 42.62, "number": 34.98, "yes/no": 76.76}}, "team_name_order": 2, "team_members": "Bolei Zhou (Facebook AI Research & MIT), Yuandong Tian (Facebook AI Research), Sainbayar Sukhbaatar (Facebook AI Research & NYU), Arthur Szlam (Facebook AI Research), Rob Fergus (Facebook AI Research).", "team_name": "Bolei", "ref": "http://visualqa.csail.mit.edu", "method": "A improved version of bag of words plus standard deep features."}, {"submissionRound": 1, "challenge": {"overall": 62.8, "perAnswerType": {"other": 51.71, "number": 36.54, "yes/no": 82.53}}, "dev": {"overall": 62.73, "perAnswerType": {"other": 51.35, "number": 38.07, "yes/no": 82.58}}, "standard": {"overall": 62.88, "perAnswerType": {"other": 51.91, "number": 37.73, "yes/no": 82.11}}, "team_name_order": 3, "team_members": "Aaditya Prakash (Brandeis University)", "team_name": "Brandeis", "ref": "http://iamaaditya.github.io/research/vqa/", "method": "We propose a variant of highway network designed to achieve multi-modal learning like VQA. We alter the signal of 'carry' gate with a multiplicand learned from word embeddings of the question. A multi-layered highway MLP learns the memory required to associate the image features with word vectors and thus achieves implicit soft attention over learned parameters."}, {"submissionRound": 1, "challenge": {"overall": 59.4, "perAnswerType": {"other": 45.8, "number": 36.39, "yes/no": 81.22}}, "dev": {"overall": 59.06, "perAnswerType": {"other": 45.17, "number": 37.71, "yes/no": 81.0}}, "standard": {"overall": 59.33, "perAnswerType": {"other": 45.76, "number": 36.44, "yes/no": 81.04}}, "team_name_order": 4, "team_members": "None", "team_name": "CNNAtt", "ref": "", "method": "CNN features and Att features"}, {"submissionRound": 4, "challenge": {"overall": 63.97, "perAnswerType": {"other": 52.62, "number": 39.18, "yes/no": 83.65}}, "dev": {"overall": 63.92, "perAnswerType": {"other": 52.32, "number": 40.66, "yes/no": 83.66}}, "standard": {"overall": 63.68, "perAnswerType": {"other": 52.09, "number": 40.07, "yes/no": 83.25}}, "team_name_order": 5, "team_members": "Gyu-tae Park* (Samsung Electronics, AI Lab),  YoungChul Sohn* (Samsung Electronics, AI Lab), Kibeom Lee* (Samsung Electronics, AI Lab), Jong-Ryul Lee* (Samsung Electronics, AI Lab)", "team_name": "DLAIT", "ref": "", "method": "Multimodal attention networks with pretrained word embedding vectors and question-specific answer prediction mechanisms."}, {"submissionRound": 2, "challenge": {"overall": 59.56, "perAnswerType": {"other": 45.96, "number": 35.14, "yes/no": 81.75}}, "dev": {"overall": 59.24, "perAnswerType": {"other": 45.77, "number": 36.16, "yes/no": 81.14}}, "standard": {"overall": 59.54, "perAnswerType": {"other": 46.1, "number": 35.67, "yes/no": 81.34}}, "team_name_order": 6, "team_members": "Ilija Ilievski (Graduate School for Integrative Sciences and Engineering,, National University of Singapore), Shuicheng Yan (Department of Electrical & Computer Engineering,, National University of Singapore), Jiashi Feng (Department of Electrical & Computer Engineering,, National University of Singapore)", "team_name": "LV-NUS", "ref": "https://arxiv.org/abs/1604.01485", "method": "We propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions, and fuse the information from the regions and global features via an LSTM unit."}, {"submissionRound": 4, "challenge": {"overall": 61.82, "perAnswerType": {"other": 49.76, "number": 36.7, "yes/no": 82.39}}, "dev": {"overall": 61.47, "perAnswerType": {"other": 49.23, "number": 37.91, "yes/no": 82.04}}, "standard": {"overall": 61.77, "perAnswerType": {"other": 49.75, "number": 37.56, "yes/no": 81.98}}, "team_name_order": 7, "team_members": "Kuniaki Saito (University of Tokyo), Andrew Shin (University of Tokyo), Yoshitaka Ushiku (University of Tokyo), Tatsuya Harada (University of Tokyo)", "team_name": "MIL-UT", "ref": "", "method": "Multimodal Dual-Network in which one network performs an addition of all input features to form a common embedding space, and the other performs multiplication. Inputs to each network consist of fc6 from VGG-19, and the uppermost fully-connected layer from Resnet-152 and Resnet-101. We implemented 19 such dual networks with varying dimensions, and averaged out."}, {"submissionRound": 2, "challenge": {"overall": 60.76, "perAnswerType": {"other": 48.74, "number": 35.9, "yes/no": 81.22}}, "dev": {"overall": 60.37, "perAnswerType": {"other": 48.25, "number": 37.0, "yes/no": 80.75}}, "standard": {"overall": 60.36, "perAnswerType": {"other": 48.33, "number": 36.82, "yes/no": 80.43}}, "team_name_order": 8, "team_members": "Caiming Xiong, Stephen Merity, Richard Socher", "team_name": "MMCX", "ref": "", "method": "CNN+GRU Memory"}, {"submissionRound": 1, "challenge": {"overall": 57.47, "perAnswerType": {"other": 42.06, "number": 35.56, "yes/no": 81.11}}, "dev": {"overall": 57.22, "perAnswerType": {"other": 41.69, "number": 37.24, "yes/no": 80.71}}, "standard": {"overall": 57.36, "perAnswerType": {"other": 42.24, "number": 36.92, "yes/no": 80.28}}, "team_name_order": 9, "team_members": "Mujtaba hasan (Indian Institute of Technology, Delhi)", "team_name": "Mujtaba hasan", "ref": "", "method": "We use finetuned VGG_19 for image representation and a novel combination of deep LSTMs and GRUs for the text analysis and train a fully connected layer on top of that for final task. We use backpropagation for end to end training and testing purposes.The weights of proposed joint network are initialized with pretrained CNN and GRU."}, {"submissionRound": 1, "challenge": {"overall": 64.89, "perAnswerType": {"other": 54.74, "number": 37.67, "yes/no": 83.78}}, "dev": {"overall": 64.93, "perAnswerType": {"other": 54.76, "number": 39.77, "yes/no": 83.48}}, "standard": {"overall": 64.79, "perAnswerType": {"other": 54.62, "number": 38.7, "yes/no": 83.31}}, "team_name_order": 10, "team_members": "Hyeonseob Nam (Naver Labs), Jeonghee Kim (Naver Labs)", "team_name": "Naver Labs", "ref": "", "method": "Dual Attention Networks (DANs) apply an attention mechanism on both image regions and question words through multiple stages. DANs focus on specific words that are relevant to the answers or the regions to attend to. 152-layer Deep Residual Network is used to extract high-level image features."}, {"submissionRound": 2, "challenge": {"overall": 63.35, "perAnswerType": {"other": 53.12, "number": 38.02, "yes/no": 81.85}}, "dev": {"overall": 63.31, "perAnswerType": {"other": 52.95, "number": 38.97, "yes/no": 81.88}}, "standard": {"overall": 63.17, "perAnswerType": {"other": 52.79, "number": 38.16, "yes/no": 81.67}}, "team_name_order": 11, "team_members": "Hyeonwoo Noh (Department of Computer Science and Engineering, POSTECH, Korea), Bohyung Han (Department of Computer Science and Engineering, POSTECH, Korea)", "team_name": "POSTECH", "ref": "http://arxiv.org/abs/1606.03647", "method": "Training Recurrent Answering Units with Joint Loss Minimization for VQA. The model is trained only on VQA dataset. ResNet101 features  are used for the representation of an image, and two-layer LSTM is employed to model questions."}, {"submissionRound": 1, "challenge": {"overall": 56.68, "perAnswerType": {"other": 42.5, "number": 35.46, "yes/no": 78.72}}, "dev": {"overall": 56.33, "perAnswerType": {"other": 41.52, "number": 36.71, "yes/no": 78.87}}, "standard": {"overall": 56.61, "perAnswerType": {"other": 42.13, "number": 35.97, "yes/no": 78.82}}, "team_name_order": 12, "team_members": "None", "team_name": "RIT", "ref": "", "method": "We created a Bayesian QDA variant and combine it with a deep neural network."}, {"submissionRound": 5, "challenge": {"overall": 61.05, "perAnswerType": {"other": 48.06, "number": 36.77, "yes/no": 82.51}}, "dev": {"overall": 60.72, "perAnswerType": {"other": 47.67, "number": 37.02, "yes/no": 82.29}}, "standard": {"overall": 60.76, "perAnswerType": {"other": 47.77, "number": 36.81, "yes/no": 82.07}}, "team_name_order": 13, "team_members": "Ruiyu Li", "team_name": "SHB_1026", "ref": "", "method": "A deep reasoning network for VQA with question representation update."}, {"submissionRound": 2, "temp_team_members": "{Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},", "challenge": {"overall": 66.9, "perAnswerType": {"other": 58.64, "number": 38.9, "yes/no": 83.79}}, "dev": {"overall": 66.67, "perAnswerType": {"other": 58.46, "number": 39.75, "yes/no": 83.4}}, "standard": {"overall": 66.47, "perAnswerType": {"other": 58.0, "number": 39.47, "yes/no": 83.24}}, "team_name_order": 14, "team_members": "Akira Fukui (UC Berkeley EECS, Sony Corp. Tokyo), Dong Huk Park (UC Berkeley EECS), Daylen Yang (UC Berkeley EECS), Anna Rohrbach (UC Berkeley EECS, Max Planck Institute for Informatics, Saarbrucken), Trevor Darrell (UC Berkeley EECS), Marcus Rohrbach (UC Berkeley EECS)", "team_name": "UC Berkeley &amp; Sony", "ref": "https://arxiv.org/abs/1606.01847", "method": "We propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation."}, {"submissionRound": 3, "challenge": {"overall": 59.43, "perAnswerType": {"other": 45.67, "number": 37.17, "yes/no": 81.24}}, "dev": {"overall": 59.36, "perAnswerType": {"other": 45.49, "number": 38.55, "yes/no": 81.12}}, "standard": {"overall": 59.44, "perAnswerType": {"other": 45.81, "number": 37.48, "yes/no": 80.98}}, "team_name_order": 15, "team_members": "Jacob Andreas (UC Berkeley), Marcus Rohrbach (UC Berkeley), Trevor Darrell (UC Berkeley), Dan Klein (UC Berkeley)", "team_name": "UC Berkeley (DNMN)", "ref": "https://github.com/jacobandreas/nmn2", "method": "Neural module network: question-answering network assembled dynamically from a collection of jointly-trained modules."}, {"submissionRound": 2, "temp_team_members": "{Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},", "challenge": {"overall": 58.65, "perAnswerType": {"other": 44.14, "number": 36.47, "yes/no": 81.32}}, "dev": {"overall": 58.62, "perAnswerType": {"other": 44.03, "number": 37.95, "yes/no": 81.19}}, "standard": {"overall": 58.66, "perAnswerType": {"other": 44.01, "number": 37.7, "yes/no": 81.16}}, "team_name_order": 16, "team_members": "Jacob Andreas (UC Berkeley), Marcus Rohrbach (UC Berkeley), Trevor Darrell (UC Berkeley), Dan Klein (UC Berkeley)", "team_name": "UC Berkeley (NMN)", "ref": "github.com/jacobandreas/nmn2", "method": "Neural module network: question-specific attentional neural network assembled from a collection of jointly-trained modules based on a fixed semantic parse."}, {"submissionRound": 3, "challenge": {"overall": 53.84, "perAnswerType": {"other": 37.1, "number": 34.5, "yes/no": 78.38}}, "dev": {"overall": 53.49, "perAnswerType": {"other": 36.65, "number": 35.58, "yes/no": 77.97}}, "standard": {"overall": 53.62, "perAnswerType": {"other": 36.7, "number": 35.53, "yes/no": 78.05}}, "team_name_order": 17, "team_members": "Issey Masuda Mora (Universitat Polit\u00e8cnica de Catalunya (UPC)), Santiago Pascual de la Puente (Universitat Polit\u00e8cnica de Catalunya (UPC)), Xavier Gir\u00f3-i-Nieto (Universitat Polit\u00e8cnica de Catalunya (UPC))", "team_name": "UPC", "ref": "http://imatge-upc.github.io/vqa-2016-cvprw/", "method": "Visual features are extracted with a Kernelized CNN [Liu 2015] and projected into the same space as the embedding of the question, which is a sentence embedding. The model it is not a classifier upon the N most frequent answers but an encoder-decoder architecture. The decoder predicts the answers word by word (the presented model only outputs one word per answer but it can be extended)."}, {"submissionRound": 1, "challenge": {"overall": 56.1, "perAnswerType": {"other": 40.79, "number": 35.72, "yes/no": 79.24}}, "dev": {"overall": 55.81, "perAnswerType": {"other": 40.16, "number": 36.74, "yes/no": 79.21}}, "standard": {"overall": 55.77, "perAnswerType": {"other": 40.27, "number": 36.33, "yes/no": 78.88}}, "team_name_order": 18, "team_members": "Marc Bola\u00f1os (Universitat de Barcelona / Computer Vision Center, Bellaterra),, \u00c1lvaro Peris (PRHLT Research Center, Universitat Polit\u00e8cnica de Val\u00e8ncia),, Petia Radeva  (Universitat de Barcelona / Computer Vision Center, Bellaterra),, Francisco Casacuberta (PRHLT Research Center, Universitat Polit\u00e8cnica de Val\u00e8ncia)", "team_name": "UPV_UB", "ref": "", "method": "Our method makes use of a Bidirectional LSTM network for processing the question and a kernelized CNN for processing the visual information. Features extracted by these models were provided to a single-layered MLP classifier over the most common 2000 answers."}, {"submissionRound": 5, "temp_team_members": "{Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},", "challenge": {"overall": 62.23, "perAnswerType": {"other": 52.16, "number": 37.87, "yes/no": 80.31}}, "dev": {"overall": 61.81, "perAnswerType": {"other": 51.77, "number": 38.65, "yes/no": 79.7}}, "standard": {"overall": 62.06, "perAnswerType": {"other": 51.95, "number": 38.22, "yes/no": 79.95}}, "team_name_order": 19, "team_members": "Jiasen Lu (Virginia Tech), Jianwei Yang (Virginia Tech), Dhruv Batra (Virginia Tech), Devi Parikh (Virginia Tech)", "team_name": "VTComputerVison", "ref": "https://arxiv.org/abs/1606.00061", "method": "We present a novel co-attention model for VQA that jointly reasons about image and question attention. And our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN) model."}, {"submissionRound": 1, "challenge": {"overall": 60.46, "perAnswerType": {"other": 47.99, "number": 37.85, "yes/no": 80.86}}, "dev": {"overall": 60.09, "perAnswerType": {"other": 47.51, "number": 38.55, "yes/no": 80.52}}, "standard": {"overall": 60.33, "perAnswerType": {"other": 47.87, "number": 38.14, "yes/no": 80.56}}, "team_name_order": 20, "team_members": "Jiasen Lu (Virginia Tech), Dhruv Batra (Virginia Tech), Devi Parikh (Virginia Tech)", "team_name": "VT_CV_Jiasen", "ref": "", "method": "Stacked Visual question answering model with 1-layer alternating question attention."}, {"submissionRound": 1, "challenge": {"overall": 55.49, "perAnswerType": {"other": 40.36, "number": 34.56, "yes/no": 78.56}}, "dev": {"overall": 55.03, "perAnswerType": {"other": 39.68, "number": 35.47, "yes/no": 78.19}}, "standard": {"overall": 55.34, "perAnswerType": {"other": 40.27, "number": 35.3, "yes/no": 78.1}}, "team_name_order": 21, "team_members": "None", "team_name": "att", "ref": "", "method": "att"}, {"submissionRound": 2, "challenge": {"overall": 58.63, "perAnswerType": {"other": 46.58, "number": 36.02, "yes/no": 78.54}}, "dev": {"overall": 58.39, "perAnswerType": {"other": 46.28, "number": 36.45, "yes/no": 78.39}}, "standard": {"overall": 58.43, "perAnswerType": {"other": 46.32, "number": 36.27, "yes/no": 78.24}}, "team_name_order": 22, "team_members": "", "team_name": "global_vision", "ref": "", "method": "Residual Net + Classification"}, {"submissionRound": 5, "challenge": {"overall": 61.59, "perAnswerType": {"other": 49.23, "number": 38.85, "yes/no": 81.9}}, "dev": {"overall": 61.48, "perAnswerType": {"other": 49.24, "number": 39.08, "yes/no": 81.74}}, "standard": {"overall": 61.69, "perAnswerType": {"other": 49.61, "number": 39.27, "yes/no": 81.53}}, "team_name_order": 23, "team_members": "Kushal Kafle (Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology), and, Christopher Kanan (Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology)", "team_name": "klab", "ref": "http://www.kushalkafle.com/kafle2016.pdf", "method": "Observing that the type of answer can be predicted from question alone, we formulated a Bayesian framework to incorporate answer-type prediction into a VQA pipeline. The current result consists of an improved MLP model trained using data augmentation.  The probabilities produced by this MLP model are then combined with a residual attention mechanism to get the predicted answers."}, {"submissionRound": 1, "challenge": {"overall": 59.14, "perAnswerType": {"other": 46.78, "number": 36.09, "yes/no": 79.53}}, "dev": {"overall": 58.68, "perAnswerType": {"other": 46.09, "number": 36.56, "yes/no": 79.28}}, "standard": {"overall": 58.85, "perAnswerType": {"other": 46.42, "number": 36.41, "yes/no": 79.11}}, "team_name_order": 24, "team_members": "None", "team_name": "san", "ref": "", "method": "san"}, {"submissionRound": 1, "challenge": {"overall": 63.4, "perAnswerType": {"other": 51.61, "number": 38.43, "yes/no": 83.64}}, "dev": {"overall": 63.17, "perAnswerType": {"other": 51.08, "number": 39.84, "yes/no": 83.51}}, "standard": {"overall": 63.18, "perAnswerType": {"other": 51.33, "number": 39.14, "yes/no": 83.16}}, "team_name_order": 25, "team_members": "Jin-Hwa Kim (Seoul National University), Sang-Woo Lee (Seoul National University), Dong-Hyun Kwak (Seoul National University), Min-Oh Heo (Seoul National University), Jeonghee Kim (Naver Labs, Naver Corp.), Jung-Woo Ha (Naver Labs, Naver Corp.), Byoung-Tak Zhang (Seoul National University)", "team_name": "snubi-naverlabs", "ref": "http://goo.gl/ZYQHR0", "method": "An ensemble of multimodal residual networks three-block layered without data augmentation. GRUs initialized with Skip-Thought Vectors for question embedding and ResNet-152 for extracting visual feature vectors are used. Joint representations are learned by element-wise multiplication, which leads to implicit attentional model without attentional parameters."}, {"submissionRound": 1, "temp_team_members": "{Jiasen Lu and Aishwarya Agrawal and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},", "challenge": {"overall": 29.88, "perAnswerType": {"other": 1.21, "number": 0.36, "yes/no": 70.97}}, "dev": {"overall": 29.66, "perAnswerType": {"other": 1.15, "number": 0.39, "yes/no": 70.81}}, "standard": {"overall": 29.72, "perAnswerType": {"other": 1.26, "number": 0.43, "yes/no": 70.53}}, "team_name_order": 26, "team_members": "Jiasen Lu (Virginia Tech), Aishwarya Agrawal (Virginia Tech), Stanislaw Antol (Virginia Tech), Margaret Mitchell (Microsoft Research), C. Lawrence Zitnick (Facebook AI Research), Dhruv Batra (Virginia Tech), Devi Parikh (Virginia Tech)", "team_name": "vqateam-all_yes", "ref": "", "method": "&quot;yes&quot; (prior) is predicted as the answer for all questions"}, {"submissionRound": 2, "temp_team_members": "{Jiasen Lu and Aishwarya Agrawal and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},", "challenge": {"overall": 58.45, "perAnswerType": {"other": 44.25, "number": 36.15, "yes/no": 80.81}}, "dev": {"overall": 57.75, "perAnswerType": {"other": 43.08, "number": 36.77, "yes/no": 80.5}}, "standard": {"overall": 58.16, "perAnswerType": {"other": 43.73, "number": 36.53, "yes/no": 80.56}}, "team_name_order": 27, "team_members": "Jiasen Lu (Virginia Tech), Aishwarya Agrawal (Virginia Tech), Stanislaw Antol (Virginia Tech), Margaret Mitchell (Microsoft Research), C. Lawrence Zitnick (Facebook AI Research), Dhruv Batra (Virginia Tech), Devi Parikh (Virginia Tech)", "team_name": "vqateam-deeperLSTM_NormlizeCNN", "ref": "", "method": "2-channel (image and question) model. Question channel (LSTM with 2 hidden layers) provides question representation and the image channel (activations from last hidden layer of VGGNet) provides image representation. The image features thus obtained are l2 normalized. Question and image features are pointwise multiplied and fed to fully connected layer to obtain softmax distribution over 1000 answers."}, {"submissionRound": 1, "temp_team_members": "{Jiasen Lu and Aishwarya Agrawal and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},", "challenge": {"overall": 54.14, "perAnswerType": {"other": 36.9, "number": 34.64, "yes/no": 79.31}}, "dev": {"overall": 53.74, "perAnswerType": {"other": 36.41, "number": 35.22, "yes/no": 78.94}}, "standard": {"overall": 54.06, "perAnswerType": {"other": 36.8, "number": 35.55, "yes/no": 79.01}}, "team_name_order": 28, "team_members": "Jiasen Lu (Virginia Tech), Aishwarya Agrawal (Virginia Tech), Stanislaw Antol (Virginia Tech), Margaret Mitchell (Microsoft Research), C. Lawrence Zitnick (Facebook AI Research), Dhruv Batra (Virginia Tech), Devi Parikh (Virginia Tech)", "team_name": "vqateam-lstm_cnn", "ref": "", "method": "2-channel (image and question) model. Question channel (LSTM with 1 hidden layer) provides question representation and the image channel (activations from last hidden layer of VGGNet) provides image representation. Question and image features are pointwise multiplied and fed to fully connected layer to obtai softmax distribution over 1000 answers."}, {"submissionRound": 1, "challenge": {"overall": 42.85, "perAnswerType": {"other": 22.1, "number": 24.23, "yes/no": 71.89}}, "dev": {"overall": 42.7, "perAnswerType": {"other": 21.94, "number": 24.36, "yes/no": 71.89}}, "standard": {"overall": 42.73, "perAnswerType": {"other": 22.0, "number": 24.31, "yes/no": 71.73}}, "team_name_order": 29, "team_members": "Aishwarya Agrawal (Virginia Tech), Jiasen Lu (Virginia Tech), Stanislaw Antol (Virginia Tech), Margaret Mitchell (Microsoft Research), C. Lawrence Zitnick (Facebook AI Research), Dhruv Batra (Virginia Tech), Devi Parikh (Virginia Tech)", "team_name": "vqateam-nearest_neighbor", "ref": "", "method": "For every question in the test set, we find its k nearest neighbor questions in the training set using cosine similarity in Skip-Thought feature space. In this set of k questions and their associated images, we find the image which is most similar to the query image in using cosine similarity in fc7 feature space. The most common ground truth answer of this most similar image and question pair is the predicted answer for the query image and question pair."}, {"submissionRound": 1, "temp_team_members": "{Jiasen Lu and Aishwarya Agrawal and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},", "challenge": {"overall": 37.47, "perAnswerType": {"other": 8.96, "number": 34.9, "yes/no": 71.4}}, "dev": {"overall": 37.54, "perAnswerType": {"other": 9.38, "number": 35.77, "yes/no": 71.03}}, "standard": {"overall": 37.55, "perAnswerType": {"other": 9.32, "number": 35.63, "yes/no": 71.17}}, "team_name_order": 30, "team_members": "Jiasen Lu (Virginia Tech), Aishwarya Agrawal (Virginia Tech), Stanislaw Antol (Virginia Tech), Margaret Mitchell (Microsoft Research), C. Lawrence Zitnick (Facebook AI Research), Dhruv Batra (Virginia Tech), Devi Parikh (Virginia Tech)", "team_name": "vqateam-prior_per_qtype", "ref": "", "method": "We pick the most common answer per question type as the predicted answer"}, {"date": "2016-06-24"}]